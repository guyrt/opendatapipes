{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, DistilBertTokenizer, DistilBertForTokenClassification\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, lines, labels, labels_to_ids):\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.texts = [tokenizer(line, padding='max_length', max_length=25, truncation=True, return_tensors=\"pt\", is_split_into_words=True) for line in lines]\n",
    "        self.labels = [create_label_array(self.texts[j].word_ids(), labels[j], labels_to_ids) for j in range(len(lines))]    \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.texts[idx]\n",
    "        batch_labels = torch.LongTensor(self.labels[idx])\n",
    "\n",
    "        return {\n",
    "            'input_ids': batch_data['input_ids'].flatten(),\n",
    "            'attention_mask': batch_data['attention_mask'].flatten(),\n",
    "            'labels': batch_labels\n",
    "        }\n",
    "    \n",
    "    \n",
    "def clean_line(line, unique_labels):\n",
    "    line = line.strip().lower()\n",
    "    line_data = [i.split('/') for i in line.split(\"\\t\")[2].split(' ') if 'fsep' not in i]\n",
    "    line_data = [list(i) for i in zip(*line_data)]\n",
    "    unique_labels.update(line_data[1])\n",
    "    \n",
    "    return line_data\n",
    "\n",
    "def create_label_array(word_ids, original_labels, labels_to_ids):\n",
    "    try:\n",
    "        t = [original_labels[i] if i is not None else -100 for i in word_ids]\n",
    "        return [labels_to_ids[tt] if tt in labels_to_ids else tt for tt in t]\n",
    "    except IndexError:\n",
    "        print(f\"Error for index {idx}\")\n",
    "        raise\n",
    "        \n",
    "\n",
    "def get_data_sequences(fh, num_lines, train_percent, seed=0):\n",
    "    random.seed(seed)\n",
    "    unique_labels = set()\n",
    "    \n",
    "    train_lines = []\n",
    "    train_labels = []\n",
    "    test_lines = []\n",
    "    test_labels = []\n",
    "    for i in range(num_lines):\n",
    "        clean_lines, clean_labels = clean_line(fh.readline(), unique_labels)\n",
    "        if random.random() < train_percent:\n",
    "            target_lines = train_lines\n",
    "            target_labels = train_labels\n",
    "        else:\n",
    "            target_lines = test_lines\n",
    "            target_labels = test_labels\n",
    "            \n",
    "        target_lines.append(clean_lines)\n",
    "        target_labels.append(clean_labels)\n",
    "\n",
    "        \n",
    "    labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "    ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "\n",
    "    return len(unique_labels), labels_to_ids, DataSequence(train_lines, train_labels, labels_to_ids), DataSequence(test_lines, test_labels, labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open(\"data/uk_openaddresses_formatted_addresses_tagged.random.tsv\", \"r\")\n",
    "num_labels, labels_to_ids, dseq_train, dseq_test = get_data_sequences(fh, 5000, 0.9, seed=20220807)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dseq_test: 532\n",
      "dseq_train: 4468\n",
      "[-100, 3, 5, 5, 5, 4, 4, 4, 4, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "7\n",
      "{'': 0, 'city': 1, 'country': 2, 'house_number': 3, 'postcode': 4, 'road': 5, 'sep': 6}\n"
     ]
    }
   ],
   "source": [
    "print(f\"dseq_test: {len(dseq_test.texts)}\")\n",
    "print(f\"dseq_train: {len(dseq_train.texts)}\")\n",
    "print(dseq_test.labels[0])\n",
    "print(num_labels)\n",
    "print(labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce564f24a9114b1fafe91d92ae243ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs: 1 | Loss:  0.380 | Accuracy:  0.000 | Val_Loss:  3.151 | Accuracy:  0.189\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea53b0204954394b1d667431157d625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6eccc39bb8a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdseq_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdseq_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-6eccc39bb8a5>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, train_dataset, val_dataset, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mtotal_loss_train\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    158\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    214\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_loop(model, train_dataset, val_dataset, batch_size):\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, num_workers=0, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = False # out of memory :(  torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for sample_batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "\n",
    "            train_label = sample_batch['labels'].to(device)\n",
    "            mask = sample_batch['attention_mask'].to(device)\n",
    "            input_id = sample_batch['input_ids'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "            # todo better loss - consider counting bad tokens\n",
    "                \n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "            total_loss_train += loss\n",
    "\n",
    "        # todo - save model\n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for test_batch in train_dataloader:\n",
    "\n",
    "            val_label = test_batch['labels'].to(device)\n",
    "            mask = test_batch['attention_mask'].to(device)\n",
    "            input_id = test_batch['input_ids'].to(device)\n",
    "\n",
    "            loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "            logits_clean = logits[val_label != -100]\n",
    "            label_clean = val_label[val_label != -100]\n",
    "\n",
    "            predictions = logits_clean.argmax(dim=1)          \n",
    "\n",
    "            acc = (predictions == label_clean).float().mean() / batch_size\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(dseq_test.texts)\n",
    "        val_loss = total_loss_val / len(dseq_test.texts)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(dseq_train.texts): .3f} | Accuracy: {total_acc_train / len(dseq_train.texts): .3f} | Val_Loss: {total_loss_val / len(dseq_test.texts): .3f} | Accuracy: {total_acc_val / len(dseq_test.texts): .3f}')\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 5\n",
    "\n",
    "model = BertModel(num_labels)\n",
    "train_loop(model, dseq_train, dseq_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab93b81dd2140e7a613fc86575f54ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs: 1 | Loss:  0.774 | Accuracy:  0.000 | Val_Loss:  6.834 | Accuracy:  1.097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f2de4c486b4111b2b70816ede3f70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs: 2 | Loss:  0.759 | Accuracy:  0.000 | Val_Loss:  6.339 | Accuracy:  1.100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cd9e3ff51740318f3df2b8281fd0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs: 3 | Loss:  0.758 | Accuracy:  0.000 | Val_Loss:  6.314 | Accuracy:  1.475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1964e0f5f444807b307bebafef0e635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs: 4 | Loss:  0.759 | Accuracy:  0.000 | Val_Loss:  6.361 | Accuracy:  1.479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8f287b7ba34a6fa0c8b054d928a37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs: 5 | Loss:  0.759 | Accuracy:  0.000 | Val_Loss:  6.349 | Accuracy:  1.477\n"
     ]
    }
   ],
   "source": [
    "# below here is debug/diagnose\n",
    "\n",
    "model = BertModel(num_labels)\n",
    "train_loop(model, dseq_train, dseq_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641e022f0b7344b4a143a8a25f4711c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4468.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertModel(num_labels)\n",
    "train_dataloader = torch.utils.data.DataLoader(dseq_train, num_workers=0, batch_size=1, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(dseq_test, num_workers=0, batch_size=1)\n",
    "\n",
    "use_cuda = False # out of memory :(  torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "best_acc = 0\n",
    "best_loss = 1000\n",
    "model.train()\n",
    "\n",
    "i= 0\n",
    "for sample_batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "    i+=1\n",
    "    if i > 1:\n",
    "        break\n",
    "    train_label = sample_batch['labels'].to(device)\n",
    "    mask = sample_batch['attention_mask'].to(device)\n",
    "    input_id = sample_batch['input_ids'].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "    # todo better loss - consider counting bad tokens\n",
    "\n",
    "    loss.sum().backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_acc_val = 0\n",
    "total_loss_val = 0\n",
    "\n",
    "for test_batch in train_dataloader:\n",
    "\n",
    "    val_label = test_batch['labels'].to(device)\n",
    "    val_mask = test_batch['attention_mask'].to(device)\n",
    "    val_input_id = test_batch['input_ids'].to(device)\n",
    "\n",
    "    test_loss, test_logits = model(val_input_id, val_mask, val_label)\n",
    "\n",
    "    logits_clean = test_logits[val_label != -100]\n",
    "    label_clean = val_label[val_label != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1)          \n",
    "\n",
    "    acc = (predictions == label_clean).float().mean()\n",
    "    total_acc_val += acc\n",
    "    total_loss_val += loss.item()\n",
    "    break\n",
    "\n",
    "val_accuracy = total_acc_val / len(dseq_test.texts)\n",
    "val_loss = total_loss_val / len(dseq_test.texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "         4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logits.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([328])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_label[val_label != -100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logits.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0226, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = sample_batch['labels'].to(device)\n",
    "mask = sample_batch['attention_mask'].to(device)\n",
    "input_id = sample_batch['input_ids'].to(device)\n",
    "loss, logits = model(input_id, mask, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814],\n",
       "        [-9.2377,  0.0756, -0.0279, -0.8616,  0.8167,  0.4795, -6.1726, -9.0814]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0345,  0.3386,  0.1161,  0.0342,  0.1419, -0.4926, -0.0822],\n",
       "        [ 0.1287,  0.1278, -0.2769, -0.0328,  0.2805, -0.2797, -0.2588],\n",
       "        [-0.1182,  0.0642,  0.2909, -0.1873, -0.0376, -0.5870,  0.6340],\n",
       "        [ 0.2929,  0.1122,  0.3456, -0.5560, -0.1414, -0.6846,  0.0358],\n",
       "        [-0.0971, -0.3623,  0.3397,  0.3219,  0.0487, -0.5433,  0.0436],\n",
       "        [-0.0797, -0.0513, -0.1123,  0.4865,  0.1557, -0.3265,  0.1303],\n",
       "        [ 0.1666, -0.1134,  0.0131, -0.5321, -0.2246, -0.1194,  0.0015],\n",
       "        [-0.1751,  0.1675,  0.0526, -0.0153,  0.0076, -0.3112, -0.3186],\n",
       "        [-0.2348,  0.1760,  0.0915,  0.1199,  0.1011, -0.3250, -0.3097],\n",
       "        [-0.1697,  0.1996,  0.0621,  0.0056, -0.0347, -0.2210, -0.3963],\n",
       "        [-0.2471,  0.1525,  0.0947,  0.1459,  0.1054, -0.3120, -0.2813],\n",
       "        [-0.1382,  0.1749,  0.0549,  0.0837,  0.0231, -0.2560, -0.2628],\n",
       "        [ 0.1957,  0.1232,  0.1402,  0.0383, -0.1520, -0.0341, -0.0914],\n",
       "        [-0.0515,  0.1926, -0.0053, -0.0053, -0.0315, -0.2154, -0.3106],\n",
       "        [-0.0093,  0.2100,  0.0542,  0.0604, -0.0372, -0.1695, -0.3557],\n",
       "        [ 0.2068,  0.0812,  0.0830,  0.0828, -0.2175, -0.0053, -0.0189],\n",
       "        [-0.1104,  0.2010,  0.0194,  0.1101,  0.0334, -0.3180, -0.2942],\n",
       "        [-0.0068,  0.2183,  0.0872,  0.0141, -0.0358, -0.1588, -0.3916],\n",
       "        [-0.1411,  0.2040,  0.0912,  0.0789, -0.0083, -0.2208, -0.3786],\n",
       "        [-0.2480,  0.2256,  0.0879,  0.2940,  0.1458, -0.2394, -0.2058],\n",
       "        [-0.2142,  0.2359,  0.1292,  0.1837,  0.1048, -0.2938, -0.3193],\n",
       "        [-0.2537,  0.1587,  0.1029,  0.1817,  0.1311, -0.3016, -0.3022],\n",
       "        [-0.0262,  0.2000,  0.1234, -0.0274,  0.0040, -0.1472, -0.4340],\n",
       "        [-0.0342,  0.2011,  0.1052, -0.0258, -0.0030, -0.1248, -0.4404],\n",
       "        [-0.1834,  0.2104,  0.0299,  0.1359,  0.0460, -0.2464, -0.1843]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'city': 1, 'country': 2, 'house_number': 3, 'postcode': 4, 'road': 5, 'sep': 6}\n"
     ]
    }
   ],
   "source": [
    "print(labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 15876,  2509, 26424,  2100,  6738,  2142,  2983,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [  101,  1015,  3614,  3702,  2346, 11503,  5677,  3051,  2142,  2983,\n",
       "         19739, 16576,  1014,  3240,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [  101,  1022,  2146,  4644,  6174, 15154,  5172,  2142,  2983,  1059,\n",
       "          2094,  2509,  1022,  6672,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [  101,  6146, 17916,  2395,  3393,  2509,  1019,  3022, 11258,  2142,\n",
       "          2983,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [  101, 15355,  2015,  2346, 22545,  4642, 16147,  1019,  2546,  2078,\n",
       "          2142,  2983,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for l in dseq_train.labels:\n",
    "    for ll in l:\n",
    "        if ll == -100:\n",
    "            continue\n",
    "        if ll not in labels:\n",
    "            labels[ll] = 0\n",
    "        labels[ll] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 3088, 5: 10157, 4: 15250, 1: 6371, 2: 6760, 6: 18}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
