{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1662929155701
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "subscription_id = 'f48a2553-c966-4d06-8faa-c5096da10254'\n",
    "resource_group = 'rg-fecdata'\n",
    "workspace_name = 'fecaml'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "datastore = workspace.datastores['fecrawzips']\n",
    "\n",
    "metadata_dataset = Dataset.get_by_name(workspace, name='fecfileformats')\n",
    "local_metadata_dataset = metadata_dataset.download()[0]\n",
    "\n",
    "output_datastore = workspace.datastores['fecparquetoutputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1662929163671
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# This is used in a notebook - will eventually be pushed into git as a notebook.\n",
    "\n",
    "import datetime\n",
    "from os.path import join, dirname\n",
    "from os import makedirs\n",
    "import tempfile\n",
    "from pyarrow import csv, parquet, string\n",
    "import ntpath\n",
    "\n",
    "\n",
    "output_delimiter = '\\t'\n",
    "\n",
    "\n",
    "class SchemaHandler(object):\n",
    "\n",
    "    def __init__(self, definitions):\n",
    "        self.feclookup = definitions\n",
    "        self.schema_cache : dict[str, dict[str, list[str]]]= {}  # file_version => (line type => schema)\n",
    "\n",
    "    def get_schema(self, version, linetype):\n",
    "        linetype = linetype.upper()\n",
    "        if linetype in ['H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7']:\n",
    "            # handle weird data bug\n",
    "            linetype = 'S' + linetype\n",
    "        \n",
    "        versioned_formdata = self.feclookup['v' + version]\n",
    "        i = len(linetype)\n",
    "\n",
    "        while i >= 0 and linetype[:i] not in versioned_formdata:\n",
    "            i -= 1\n",
    "\n",
    "        if not linetype:\n",
    "            raise Exception(\"Could not match linetype {0} on version {1}\".format(linetype, version))\n",
    "\n",
    "        final_key = linetype[:i]\n",
    "        return final_key, versioned_formdata.get(final_key, dict())\n",
    "\n",
    "    def get_schema_string(self, fileversion, clean_linetype):\n",
    "        if fileversion not in self.schema_cache:\n",
    "            self.schema_cache[fileversion] = {}\n",
    "\n",
    "        if clean_linetype not in self.schema_cache[fileversion]:\n",
    "            if clean_linetype == 'error':\n",
    "                self.schema_cache[fileversion][clean_linetype] = output_delimiter.join(['clean_linetype', 'upload_date', 'linetype', 'error', 'filename']).encode()\n",
    "            else:\n",
    "                _, schema = self.get_schema(fileversion, clean_linetype)\n",
    "                schema = list(schema)\n",
    "                schema.insert(0, 'upload_date')\n",
    "                schema.insert(0, 'clean_linetype')\n",
    "                schema.append('filename')\n",
    "                self.schema_cache[fileversion][clean_linetype] = output_delimiter.join(schema).encode()\n",
    "\n",
    "        final_value = self.schema_cache[fileversion][clean_linetype]\n",
    "        return final_value\n",
    "\n",
    "\n",
    "class LowMemoryFecFileParser(object):\n",
    "    \"\"\"\n",
    "    Given a file from the FEC, apply correct definitions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema_handler, upload_date, line_aggregator):\n",
    "        self.schema_handler = schema_handler\n",
    "        self.upload_date = upload_date\n",
    "        self.line_aggregator = line_aggregator\n",
    "        self.schema_cache : dict[str, dict[str, list[str]]]= {}  # file_version => (line type => schema)\n",
    "\n",
    "    def processfile(self, filehandle, filename):\n",
    "        \"\"\"\n",
    "        Process all lines of a file and list of dictionaries, one per line.\n",
    "        \"\"\"\n",
    "        first_line = filehandle.readline()\n",
    "        first_line = first_line.replace('\"', '').strip().split(chr(28))\n",
    "        if first_line[0] != \"HDR\":\n",
    "            raise Exception(\"Failed to parse: HDR expected on first line\")\n",
    "\n",
    "        fileversion = first_line[2].strip()\n",
    "\n",
    "        in_comment = False\n",
    "\n",
    "        for line in filehandle:\n",
    "            line = line.strip()\n",
    "            line = line.replace('\"', '')\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if line == '[BEGINTEXT]':\n",
    "                in_comment = True\n",
    "                continue\n",
    "            elif in_comment:\n",
    "                if line == '[ENDTEXT]':\n",
    "                    in_comment = False\n",
    "                continue\n",
    "\n",
    "            line : list[str] = line.split(chr(28))\n",
    "            line = [l.replace(output_delimiter, ' ') for l in line]\n",
    "            linetype = line[0]\n",
    "            clean_linetype, schema = self.schema_handler.get_schema(fileversion, linetype)\n",
    "            \n",
    "            # Send the line to right place.\n",
    "            if schema:\n",
    "                if len(schema) < len(line):\n",
    "                    line = line[:len(schema)]\n",
    "                while len(line) < len(schema):\n",
    "                    line.append('')\n",
    "\n",
    "                line.insert(0, self.upload_date)\n",
    "                line.insert(0, clean_linetype)\n",
    "                line.append(ntpath.basename(filename))\n",
    "            else:\n",
    "                clean_linetype = 'error'\n",
    "                print(f\"Error row: {line}\")\n",
    "                line = [clean_linetype, self.upload_date, linetype, \"NoSchema\", filename]\n",
    "\n",
    "            line_out = output_delimiter.join(line)\n",
    "            self.write_line(clean_linetype, line_out, fileversion)\n",
    "\n",
    "    def summarize_schema_cache(self):\n",
    "        for file_version, cache in self.schema_cache.items():\n",
    "            for line_type, final_cache in cache.items():\n",
    "                cache_s = final_cache.decode().split(output_delimiter)\n",
    "                cache_size = len(cache_s)\n",
    "                print(f\"{file_version}, {line_type}, {cache_size}\")\n",
    "\n",
    "    def write_line(self, clean_linetype : str, line : str, fileversion : str):\n",
    "        self.line_aggregator.write(clean_linetype, line, fileversion)\n",
    "\n",
    "    def finalize(self):\n",
    "        self.line_aggregator.finalize()\n",
    "\n",
    "\n",
    "class LineAggregator(object):\n",
    "\n",
    "    def __init__(self, schema_handler, dateprefix, converter_handler):\n",
    "        self.schema_handler = schema_handler\n",
    "        self.dateprefix = dateprefix\n",
    "        self.file_pointers : dict[str, tempfile.TemporaryFile] = {}  # lineType -> fp\n",
    "        self.file_sizes : dict[str, int] = {}  # lineType -> current file size \n",
    "        self.converter_handler = converter_handler\n",
    "\n",
    "        self.file_size_limit = 1024 * 1024 * 200\n",
    "\n",
    "    def write(self, clean_linetype : str, line : str, file_version: str):\n",
    "        # Note that original_schema does not contain our added columns.\n",
    "        # It's used in converter to force null types to behave. Our added columns already do.\n",
    "        schema_str = self.schema_handler.get_schema_string(file_version, clean_linetype)\n",
    "        if not self._get_file(file_version, clean_linetype):\n",
    "            self._set_file(file_version, clean_linetype, schema_str)\n",
    "\n",
    "        line = line + '\\n'\n",
    "        line = line.encode()\n",
    "        self.file_sizes[clean_linetype] += self._get_file(file_version, clean_linetype).write(line)\n",
    "        \n",
    "        if self.file_sizes[clean_linetype] > self.file_size_limit:\n",
    "            _, original_schema = self.schema_handler.get_schema(file_version, clean_linetype)\n",
    "            self.converter_handler.convert(clean_linetype, self._get_file(file_version, clean_linetype), original_schema)\n",
    "            self._set_file(file_version, clean_linetype, schema_str)\n",
    "\n",
    "    def _get_file(self, file_version : str, clean_linetype : str):\n",
    "        version_pointers = self.file_pointers.get(file_version)\n",
    "        if not version_pointers:\n",
    "            return None\n",
    "        return version_pointers.get(clean_linetype)\n",
    "\n",
    "    def _set_file(self, fileversion : str, clean_linetype : str, schema_str : str):\n",
    "        if fileversion not in self.file_pointers:\n",
    "            self.file_pointers[fileversion] = {}\n",
    "\n",
    "        if clean_linetype in self.file_pointers[fileversion]:\n",
    "            del self.file_pointers[fileversion][clean_linetype]\n",
    "\n",
    "        local_file_handle = tempfile.TemporaryFile()\n",
    "\n",
    "        self.file_sizes[clean_linetype] = local_file_handle.write(schema_str)\n",
    "        local_file_handle.write('\\n'.encode())\n",
    "        self.file_pointers[fileversion][clean_linetype] = local_file_handle\n",
    "        \n",
    "        return local_file_handle\n",
    "\n",
    "    def finalize(self):\n",
    "        for fileversion, pointers in self.file_pointers.items():\n",
    "            for clean_linetype, file_pointer in pointers.items():\n",
    "                _, original_schema = self.schema_handler.get_schema(fileversion, clean_linetype)\n",
    "                self.converter_handler.convert(clean_linetype, file_pointer, original_schema)\n",
    "        self.file_pointers = {}\n",
    "        self.file_sizes = {}\n",
    "\n",
    "\n",
    "class ParquetConverter(object):\n",
    "\n",
    "    def __init__(self, root_folder, date_pattern) -> None:\n",
    "        self.root_folder = root_folder\n",
    "        self.date_pattern = date_pattern\n",
    "        self.counter = 0\n",
    "        self.files = {}  # line type => filenames\n",
    "\n",
    "    def convert(self, line_type: str, file_pointer, original_schema):\n",
    "        \"\"\"Convert a delimited file to parquet\"\"\"\n",
    "        file_pointer.flush()\n",
    "        file_pointer.seek(0)\n",
    "\n",
    "        column_opts_dict = {}\n",
    "        for col in original_schema:\n",
    "            column_opts_dict[col] = string()\n",
    "\n",
    "        try:\n",
    "            df = csv.read_csv(\n",
    "                file_pointer, \n",
    "                parse_options=csv.ParseOptions(delimiter=output_delimiter), \n",
    "                convert_options=csv.ConvertOptions(column_types=column_opts_dict)\n",
    "            )\n",
    "        except Exception:\n",
    "            print(f\"Failed on {line_type}. Dumping\")\n",
    "\n",
    "            emh = open('/tmp/emergency_dump.csv', 'wb')\n",
    "            file_pointer.seek(0)\n",
    "            emh.write(file_pointer.read())\n",
    "            emh.close()\n",
    "\n",
    "            raise\n",
    "\n",
    "        local_filename = join(self.root_folder, line_type, f'{self.date_pattern}_{self.counter}.snappy.parquet')\n",
    "        self.counter += 1\n",
    "        makedirs(dirname(local_filename), exist_ok=True)\n",
    "        new_fp = open(local_filename, 'wb')\n",
    "\n",
    "        parquet.write_table(df, new_fp, flavor='spark')\n",
    "        new_fp.close()\n",
    "        return local_filename   \n",
    "\n",
    "\n",
    "    def consolidate(self):\n",
    "        \"\"\"For each line type, consolidate the folder to a single parquet file and remove nulls\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def build_parser(fec_definitions, parquet_root, date_pattern):\n",
    "    utc_timestamp = str(datetime.datetime.utcnow())\n",
    "    schema_handler = SchemaHandler(fec_definitions)\n",
    "    parquet_convert = ParquetConverter(parquet_root, date_pattern)\n",
    "    line_aggregator = LineAggregator(schema_handler, date_pattern, parquet_convert)\n",
    "    return LowMemoryFecFileParser(schema_handler, utc_timestamp, line_aggregator)\n",
    "\n",
    "\n",
    "def upload(local_path):\n",
    "    remote_path = f'amluploads/electronic/'\n",
    "    print(f'Uploading {local_path} to {remote_path}')\n",
    "    output_datastore.upload(local_path, remote_path, overwrite=True, show_progress=False)\n",
    "\n",
    "\n",
    "def get_tempdir(reason):\n",
    "    td = tempfile.TemporaryDirectory()\n",
    "    print(f\"Got {td.name} for {reason}\")\n",
    "    return td\n",
    "\n",
    "def process_file(files, datepattern, unzip_tempdir):\n",
    "    parquet_folder = get_tempdir(\"parquet_folder\")\n",
    "    parser = build_parser(fec_definitions, parquet_folder.name, datepattern)\n",
    "\n",
    "    print(f\"Starting to operate on {len(files)} files\")\n",
    "    try:\n",
    "        for rawfilename in files:\n",
    "            # run in blocks of 100 files, or otherwise watch for full disk. On full/100 then run convert/upload then cleanup/recreate\n",
    "            # your temp space.\n",
    "            filename = join(unzip_tempdir.name, rawfilename)\n",
    "            fh = open(filename, 'r', errors='ignore')\n",
    "            parser.processfile(fh, filename)\n",
    "        parser.finalize()\n",
    "        upload(parquet_folder.name)\n",
    "    finally:\n",
    "        print(\"Cleaning up\")\n",
    "        parquet_folder.cleanup()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1662929362989
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 20210420\n",
      "Got /tmp/tmpxwhg2kln for rawdownload\n",
      "Got /tmp/tmpau8y4345 for parquet_folder\n",
      "Starting to operate on 614 files\n",
      "Uploading /tmp/tmpau8y4345 to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210421\n",
      "Got /tmp/tmpqmqke7d_ for rawdownload\n",
      "Got /tmp/tmp82m3vlh9 for parquet_folder\n",
      "Starting to operate on 170 files\n",
      "Uploading /tmp/tmp82m3vlh9 to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210422\n",
      "Got /tmp/tmpx4ukmhlh for rawdownload\n",
      "Got /tmp/tmptnz4y8nz for parquet_folder\n",
      "Starting to operate on 109 files\n",
      "Uploading /tmp/tmptnz4y8nz to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210423\n",
      "Got /tmp/tmp0ehlslkb for rawdownload\n",
      "Got /tmp/tmpt2rv4l7g for parquet_folder\n",
      "Starting to operate on 95 files\n",
      "Uploading /tmp/tmpt2rv4l7g to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210424\n",
      "Got /tmp/tmpjjgpuody for rawdownload\n",
      "Got /tmp/tmptdlkb8ms for parquet_folder\n",
      "Starting to operate on 33 files\n",
      "Uploading /tmp/tmptdlkb8ms to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210425\n",
      "Got /tmp/tmph86zk6sq for rawdownload\n",
      "Got /tmp/tmpag9mf_vz for parquet_folder\n",
      "Starting to operate on 6 files\n",
      "Uploading /tmp/tmpag9mf_vz to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210426\n",
      "Got /tmp/tmporyvdgvs for rawdownload\n",
      "Got /tmp/tmparm29j2h for parquet_folder\n",
      "Starting to operate on 88 files\n",
      "Uploading /tmp/tmparm29j2h to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210427\n",
      "Got /tmp/tmp6qvblqu6 for rawdownload\n",
      "Got /tmp/tmph8jldl3c for parquet_folder\n",
      "Starting to operate on 97 files\n",
      "Uploading /tmp/tmph8jldl3c to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210428\n",
      "Got /tmp/tmpzdujzian for rawdownload\n",
      "Got /tmp/tmpwu0fueyf for parquet_folder\n",
      "Starting to operate on 117 files\n",
      "Uploading /tmp/tmpwu0fueyf to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210429\n",
      "Got /tmp/tmp4v3m_v9t for rawdownload\n",
      "Got /tmp/tmpd2udzmhz for parquet_folder\n",
      "Starting to operate on 115 files\n",
      "Uploading /tmp/tmpd2udzmhz to amluploads/electronic/\n",
      "Cleaning up\n",
      "Working on 20210430\n",
      "Got /tmp/tmp5f5ewe3j for rawdownload\n",
      "Got /tmp/tmpxdamkkbq for parquet_folder\n",
      "Starting to operate on 78 files\n",
      "Uploading /tmp/tmpxdamkkbq to amluploads/electronic/\n",
      "Cleaning up\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import json\n",
    "\n",
    "import zipfile\n",
    "import tempfile\n",
    "import datetime\n",
    "\n",
    "dt = datetime.datetime(2021, 4, 20)\n",
    "while dt < datetime.datetime(2021, 5, 1):\n",
    "    datepattern = datetime.datetime.strftime(dt, '%Y%m%d')\n",
    "    print(f\"Working on {datepattern}\")\n",
    "\n",
    "    ds = Dataset.File.from_files((datastore, f'electronic/{datepattern}.zip'))\n",
    "    downloaded_files = ds.download()\n",
    "\n",
    "    unzip_tempdir = get_tempdir(\"rawdownload\")\n",
    "    with zipfile.ZipFile(open(downloaded_files[0], 'rb')) as zipObj:\n",
    "        zipObj.extractall(unzip_tempdir.name)\n",
    "\n",
    "    fec_definitions = json.loads(open(local_metadata_dataset, 'r').read())\n",
    "    files = listdir(unzip_tempdir.name)\n",
    "    process_file(files, datepattern, unzip_tempdir)\n",
    "    unzip_tempdir.cleanup()\n",
    "\n",
    "    dt += datetime.timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
