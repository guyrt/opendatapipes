{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset, Datastore\r\n",
        "\r\n",
        "subscription_id = '<secret>'\r\n",
        "resource_group = '<secret>'\r\n",
        "workspace_name = '<secret>'\r\n",
        "\r\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\r\n",
        "\r\n",
        "datastore = workspace.datastores['fecrawzips']\r\n",
        "\r\n",
        "metadata_dataset = Dataset.get_by_name(workspace, name='fecfileformats')\r\n",
        "local_metadata_dataset = metadata_dataset.download()[0]\r\n",
        "\r\n",
        "output_datastore = workspace.datastores['fecparquetoutputs']\r\n",
        "type(output_datastore)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "azureml.data.azure_storage_datastore.AzureBlobDatastore"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1657915538173
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is used in a notebook - will eventually be pushed into git as a notebook.\r\n",
        "\r\n",
        "import datetime\r\n",
        "from os.path import join, dirname\r\n",
        "from os import makedirs\r\n",
        "import tempfile\r\n",
        "from pyarrow import csv, parquet, string\r\n",
        "\r\n",
        "output_delimiter = '\\t'\r\n",
        "\r\n",
        "\r\n",
        "class SchemaHandler(object):\r\n",
        "\r\n",
        "    def __init__(self, definitions):\r\n",
        "        self.feclookup = definitions\r\n",
        "        self.schema_cache : dict[str, dict[str, list[str]]]= {}  # file_version => (line type => schema)\r\n",
        "\r\n",
        "    def get_schema(self, version, linetype):\r\n",
        "        linetype = linetype.upper()\r\n",
        "        if linetype in ['H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7']:\r\n",
        "            # handle weird data bug\r\n",
        "            linetype = 'S' + linetype\r\n",
        "        \r\n",
        "        versioned_formdata = self.feclookup['v' + version]\r\n",
        "        i = len(linetype)\r\n",
        "\r\n",
        "        while i >= 0 and linetype[:i] not in versioned_formdata:\r\n",
        "            i -= 1\r\n",
        "\r\n",
        "        if not linetype:\r\n",
        "            raise Exception(\"Could not match linetype {0} on version {1}\".format(linetype, version))\r\n",
        "\r\n",
        "        final_key = linetype[:i]\r\n",
        "        return final_key, versioned_formdata.get(final_key, dict())\r\n",
        "\r\n",
        "    def get_schema_string(self, fileversion, clean_linetype):\r\n",
        "        if fileversion not in self.schema_cache:\r\n",
        "            self.schema_cache[fileversion] = {}\r\n",
        "\r\n",
        "        if clean_linetype not in self.schema_cache[fileversion]:\r\n",
        "            if clean_linetype == 'error':\r\n",
        "                self.schema_cache[fileversion][clean_linetype] = output_delimiter.join(['clean_linetype', 'upload_date', 'linetype', 'error', 'filename']).encode()\r\n",
        "            else:\r\n",
        "                _, schema = self.get_schema(fileversion, clean_linetype)\r\n",
        "                schema = list(schema)\r\n",
        "                schema.insert(0, 'upload_date')\r\n",
        "                schema.insert(0, 'clean_linetype')\r\n",
        "                self.schema_cache[fileversion][clean_linetype] = output_delimiter.join(schema).encode()\r\n",
        "\r\n",
        "        final_value = self.schema_cache[fileversion][clean_linetype]\r\n",
        "        return final_value\r\n",
        "\r\n",
        "\r\n",
        "class LowMemoryFecFileParser(object):\r\n",
        "    \"\"\"\r\n",
        "    Given a file from the FEC, apply correct definitions.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, schema_handler, upload_date, line_aggregator):\r\n",
        "        self.schema_handler = schema_handler\r\n",
        "        self.upload_date = upload_date\r\n",
        "        self.line_aggregator = line_aggregator\r\n",
        "        self.schema_cache : dict[str, dict[str, list[str]]]= {}  # file_version => (line type => schema)\r\n",
        "\r\n",
        "    def processfile(self, filehandle, filename):\r\n",
        "        \"\"\"\r\n",
        "        Process all lines of a file and list of dictionaries, one per line.\r\n",
        "        \"\"\"\r\n",
        "        first_line = filehandle.readline()\r\n",
        "        first_line = first_line.replace('\"', '').strip().split(chr(28))\r\n",
        "        if first_line[0] != \"HDR\":\r\n",
        "            raise Exception(\"Failed to parse: HDR expected on first line\")\r\n",
        "\r\n",
        "        fileversion = first_line[2].strip()\r\n",
        "\r\n",
        "        in_comment = False\r\n",
        "\r\n",
        "        for line in filehandle:\r\n",
        "            line = line.strip()\r\n",
        "            line = line.replace('\"', '')\r\n",
        "\r\n",
        "            if not line:\r\n",
        "                continue\r\n",
        "            \r\n",
        "            if line == '[BEGINTEXT]':\r\n",
        "                in_comment = True\r\n",
        "                continue\r\n",
        "            elif in_comment:\r\n",
        "                if line == '[ENDTEXT]':\r\n",
        "                    in_comment = False\r\n",
        "                continue\r\n",
        "\r\n",
        "            line : list[str] = line.split(chr(28))\r\n",
        "            line = [l.replace(output_delimiter, ' ') for l in line]\r\n",
        "            linetype = line[0]\r\n",
        "            clean_linetype, schema = self.schema_handler.get_schema(fileversion, linetype)\r\n",
        "            \r\n",
        "            # Send the line to right place.\r\n",
        "            if schema:\r\n",
        "                if len(schema) < len(line):\r\n",
        "                    line = line[:len(schema)]\r\n",
        "                while len(line) < len(schema):\r\n",
        "                    line.append('')\r\n",
        "\r\n",
        "                line.insert(0, self.upload_date)\r\n",
        "                line.insert(0, clean_linetype)\r\n",
        "            else:\r\n",
        "                clean_linetype = 'error'\r\n",
        "                print(f\"Error row: {line}\")\r\n",
        "                line = [clean_linetype, self.upload_date, linetype, \"NoSchema\", filename]\r\n",
        "\r\n",
        "            line_out = output_delimiter.join(line)\r\n",
        "            self.write_line(clean_linetype, line_out, fileversion)\r\n",
        "\r\n",
        "    def summarize_schema_cache(self):\r\n",
        "        for file_version, cache in self.schema_cache.items():\r\n",
        "            for line_type, final_cache in cache.items():\r\n",
        "                cache_s = final_cache.decode().split(output_delimiter)\r\n",
        "                cache_size = len(cache_s)\r\n",
        "                print(f\"{file_version}, {line_type}, {cache_size}\")\r\n",
        "\r\n",
        "    def write_line(self, clean_linetype : str, line : str, fileversion : str):\r\n",
        "        self.line_aggregator.write(clean_linetype, line, fileversion)\r\n",
        "\r\n",
        "    def finalize(self):\r\n",
        "        self.line_aggregator.finalize()\r\n",
        "\r\n",
        "\r\n",
        "class LineAggregator(object):\r\n",
        "\r\n",
        "    def __init__(self, schema_handler, dateprefix, converter_handler):\r\n",
        "        self.schema_handler = schema_handler\r\n",
        "        self.dateprefix = dateprefix\r\n",
        "        self.file_pointers : dict[str, tempfile.TemporaryFile] = {}  # lineType -> fp\r\n",
        "        self.file_sizes : dict[str, int] = {}  # lineType -> current file size \r\n",
        "        self.converter_handler = converter_handler\r\n",
        "\r\n",
        "        self.file_size_limit = 1024 * 1024 * 200\r\n",
        "\r\n",
        "    def write(self, clean_linetype : str, line : str, file_version: str):\r\n",
        "        # Note that original_schema does not contain our added columns.\r\n",
        "        # It's used in converter to force null types to behave. Our added columns already do.\r\n",
        "        schema_str = self.schema_handler.get_schema_string(file_version, clean_linetype)\r\n",
        "        if not self._get_file(file_version, clean_linetype):\r\n",
        "            self._set_file(file_version, clean_linetype, schema_str)\r\n",
        "\r\n",
        "        line = line + '\\n'\r\n",
        "        line = line.encode()\r\n",
        "        self.file_sizes[clean_linetype] += self._get_file(file_version, clean_linetype).write(line)\r\n",
        "        \r\n",
        "        if self.file_sizes[clean_linetype] > self.file_size_limit:\r\n",
        "            _, original_schema = self.schema_handler.get_schema(file_version, clean_linetype)\r\n",
        "            self.converter_handler.convert(clean_linetype, self._get_file(file_version, clean_linetype), original_schema)\r\n",
        "            self._set_file(file_version, clean_linetype, schema_str)\r\n",
        "\r\n",
        "    def _get_file(self, file_version : str, clean_linetype : str):\r\n",
        "        version_pointers = self.file_pointers.get(file_version)\r\n",
        "        if not version_pointers:\r\n",
        "            return None\r\n",
        "        return version_pointers.get(clean_linetype)\r\n",
        "\r\n",
        "    def _set_file(self, fileversion : str, clean_linetype : str, schema_str : str):\r\n",
        "        if fileversion not in self.file_pointers:\r\n",
        "            self.file_pointers[fileversion] = {}\r\n",
        "\r\n",
        "        if clean_linetype in self.file_pointers[fileversion]:\r\n",
        "            del self.file_pointers[fileversion][clean_linetype]\r\n",
        "\r\n",
        "        local_file_handle = tempfile.TemporaryFile()\r\n",
        "\r\n",
        "        self.file_sizes[clean_linetype] = local_file_handle.write(schema_str)\r\n",
        "        local_file_handle.write('\\n'.encode())\r\n",
        "        self.file_pointers[fileversion][clean_linetype] = local_file_handle\r\n",
        "        \r\n",
        "        return local_file_handle\r\n",
        "\r\n",
        "    def finalize(self):\r\n",
        "        for fileversion, pointers in self.file_pointers.items():\r\n",
        "            for clean_linetype, file_pointer in pointers.items():\r\n",
        "                _, original_schema = self.schema_handler.get_schema(fileversion, clean_linetype)\r\n",
        "                self.converter_handler.convert(clean_linetype, file_pointer, original_schema)\r\n",
        "        self.file_pointers = {}\r\n",
        "        self.file_sizes = {}\r\n",
        "\r\n",
        "\r\n",
        "class ParquetConverter(object):\r\n",
        "\r\n",
        "    def __init__(self, root_folder, date_pattern) -> None:\r\n",
        "        self.root_folder = root_folder\r\n",
        "        self.date_pattern = date_pattern\r\n",
        "        self.counter = 0\r\n",
        "        self.files = {}  # line type => filenames\r\n",
        "\r\n",
        "    def convert(self, line_type: str, file_pointer, original_schema):\r\n",
        "        \"\"\"Convert a delimited file to parquet\"\"\"\r\n",
        "        file_pointer.flush()\r\n",
        "        file_pointer.seek(0)\r\n",
        "\r\n",
        "        column_opts_dict = {}\r\n",
        "        for col in original_schema:\r\n",
        "            column_opts_dict[col] = string()\r\n",
        "\r\n",
        "        try:\r\n",
        "            df = csv.read_csv(\r\n",
        "                file_pointer, \r\n",
        "                parse_options=csv.ParseOptions(delimiter=output_delimiter), \r\n",
        "                convert_options=csv.ConvertOptions(column_types=column_opts_dict)\r\n",
        "            )\r\n",
        "        except Exception:\r\n",
        "            print(f\"Failed on {line_type}. Dumping\")\r\n",
        "\r\n",
        "            emh = open('/tmp/emergency_dump.csv', 'wb')\r\n",
        "            file_pointer.seek(0)\r\n",
        "            emh.write(file_pointer.read())\r\n",
        "            emh.close()\r\n",
        "\r\n",
        "            raise\r\n",
        "\r\n",
        "        local_filename = join(self.root_folder, line_type, f'{self.date_pattern}_{self.counter}.snappy.parquet')\r\n",
        "        self.counter += 1\r\n",
        "        makedirs(dirname(local_filename), exist_ok=True)\r\n",
        "        new_fp = open(local_filename, 'wb')\r\n",
        "\r\n",
        "        parquet.write_table(df, new_fp, flavor='spark')\r\n",
        "        new_fp.close()\r\n",
        "        return local_filename   \r\n",
        "\r\n",
        "\r\n",
        "    def consolidate(self):\r\n",
        "        \"\"\"For each line type, consolidate the folder to a single parquet file and remove nulls\"\"\"\r\n",
        "        pass\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def build_parser(fec_definitions, parquet_root, date_pattern):\r\n",
        "    utc_timestamp = str(datetime.datetime.utcnow())\r\n",
        "    schema_handler = SchemaHandler(fec_definitions)\r\n",
        "    parquet_convert = ParquetConverter(parquet_root, date_pattern)\r\n",
        "    line_aggregator = LineAggregator(schema_handler, date_pattern, parquet_convert)\r\n",
        "    return LowMemoryFecFileParser(schema_handler, utc_timestamp, line_aggregator)\r\n",
        "\r\n",
        "\r\n",
        "def upload(local_path):\r\n",
        "    remote_path = f'amluploads/electronic/'\r\n",
        "    print(f'Uploading {local_path} to {remote_path}')\r\n",
        "    output_datastore.upload(local_path, remote_path, overwrite=True, show_progress=False)\r\n",
        "\r\n",
        "\r\n",
        "def get_tempdir(reason):\r\n",
        "    td = tempfile.TemporaryDirectory()\r\n",
        "    print(f\"Got {td.name} for {reason}\")\r\n",
        "    return td\r\n",
        "\r\n",
        "def process_file(files, datepattern, unzip_tempdir):\r\n",
        "    parquet_folder = get_tempdir(\"parquet_folder\")\r\n",
        "    parser = build_parser(fec_definitions, parquet_folder.name, datepattern)\r\n",
        "\r\n",
        "    print(f\"Starting to operate on {len(files)} files\")\r\n",
        "    try:\r\n",
        "        for rawfilename in files:\r\n",
        "            # run in blocks of 100 files, or otherwise watch for full disk. On full/100 then run convert/upload then cleanup/recreate\r\n",
        "            # your temp space.\r\n",
        "            filename = join(unzip_tempdir.name, rawfilename)\r\n",
        "            fh = open(filename, 'r', errors='ignore')\r\n",
        "            parser.processfile(fh, filename)\r\n",
        "        parser.finalize()\r\n",
        "        upload(parquet_folder.name)\r\n",
        "    finally:\r\n",
        "        print(\"Cleaning up\")\r\n",
        "        parquet_folder.cleanup()\r\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1657915546631
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\r\n",
        "import json\r\n",
        "\r\n",
        "import zipfile\r\n",
        "import tempfile\r\n",
        "import datetime\r\n",
        "\r\n",
        "dt = datetime.datetime(2022, 2, 1)\r\n",
        "while dt < datetime.datetime(2022, 2, 10):\r\n",
        "    datepattern = datetime.datetime.strftime(dt, '%Y%m%d')\r\n",
        "    print(f\"Working on {datepattern}\")\r\n",
        "\r\n",
        "    ds = Dataset.File.from_files((datastore, f'electronic/{datepattern}.zip'))\r\n",
        "    downloaded_files = ds.download()\r\n",
        "\r\n",
        "    unzip_tempdir = get_tempdir(\"rawdownload\")\r\n",
        "    with zipfile.ZipFile(open(downloaded_files[0], 'rb')) as zipObj:\r\n",
        "        zipObj.extractall(unzip_tempdir.name)\r\n",
        "\r\n",
        "    fec_definitions = json.loads(open(local_metadata_dataset, 'r').read())\r\n",
        "    files = listdir(unzip_tempdir.name)\r\n",
        "    process_file(files, datepattern, unzip_tempdir)\r\n",
        "    unzip_tempdir.cleanup()\r\n",
        "\r\n",
        "    dt += datetime.timedelta(days=1)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\"Datastore.upload\" is deprecated after version 1.0.69. Please use \"Dataset.File.upload_directory\" to upload your files             from a local directory and create FileDataset in single method call. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading /tmp/tmpc18u2egi to amluploads/electronic/\nCleaning up\nWorking on 20220201\nGot /tmp/tmpsuwe0yvr for rawdownload\nGot /tmp/tmp6lhszu0h for parquet_folder\nStarting to operate on 1144 files\nWorking on 20220201\nWorking on 20220201\nGot /tmp/tmp2nwtxjtc for rawdownload\nGot /tmp/tmp6hfpeqky for parquet_folder\nStarting to operate on 1144 files\nUploading /tmp/tmp6hfpeqky to amluploads/electronic/\nCleaning up\nWorking on 20220202\nGot /tmp/tmp4rhaizl1 for rawdownload\nGot /tmp/tmpftghpdxw for parquet_folder\nStarting to operate on 276 files\nUploading /tmp/tmpftghpdxw to amluploads/electronic/\nCleaning up\nWorking on 20220203\nGot /tmp/tmppked8kgw for rawdownload\nGot /tmp/tmp_74jmmr9 for parquet_folder\nStarting to operate on 241 files\nUploading /tmp/tmp_74jmmr9 to amluploads/electronic/\nCleaning up\nWorking on 20220204\nGot /tmp/tmpadh5mau0 for rawdownload\nGot /tmp/tmphdkhp_dv for parquet_folder\nStarting to operate on 166 files\nUploading /tmp/tmphdkhp_dv to amluploads/electronic/\nCleaning up\nWorking on 20220205\nGot /tmp/tmpvt9zo6ks for rawdownload\nGot /tmp/tmpu0v_76v7 for parquet_folder\nStarting to operate on 41 files\nUploading /tmp/tmpu0v_76v7 to amluploads/electronic/\nCleaning up\nWorking on 20220206\nGot /tmp/tmpf_xzpe30 for rawdownload\nGot /tmp/tmptmdj84nt for parquet_folder\nStarting to operate on 38 files\nUploading /tmp/tmptmdj84nt to amluploads/electronic/\nCleaning up\nWorking on 20220207\nGot /tmp/tmpj3dz4mk7 for rawdownload\nGot /tmp/tmpdd3xjysz for parquet_folder\nStarting to operate on 138 files\nUploading /tmp/tmpdd3xjysz to amluploads/electronic/\nCleaning up\nWorking on 20220208\nGot /tmp/tmpf5edxowb for rawdownload\nGot /tmp/tmp2i5pcnum for parquet_folder\nStarting to operate on 181 files\nUploading /tmp/tmp2i5pcnum to amluploads/electronic/\nCleaning up\nWorking on 20220209\nGot /tmp/tmpntlsr7vl for rawdownload\nGot /tmp/tmprjb1g9k0 for parquet_folder\nStarting to operate on 200 files\nUploading /tmp/tmprjb1g9k0 to amluploads/electronic/\nCleaning up\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1657916831474
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = parquet.read_table(file_list[2])\r\n",
        "df2 = parquet.read_table(file_list[0])"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1657912477624
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1657915507501
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}