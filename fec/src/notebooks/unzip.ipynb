{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset, Datastore\r\n",
        "\r\n",
        "subscription_id = 'f48a2553-c966-4d06-8faa-c5096da10254'\r\n",
        "resource_group = 'rg-fecdata'\r\n",
        "workspace_name = 'fecaml'\r\n",
        "\r\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\r\n",
        "\r\n",
        "datastore = workspace.datastores['fecrawzips']\r\n",
        "\r\n",
        "metadata_dataset = Dataset.get_by_name(workspace, name='fecfileformats')\r\n",
        "local_metadata_dataset = metadata_dataset.download()[0]\r\n",
        "\r\n",
        "output_datastore = workspace.datastores['fecparquetoutputs']"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1662929155701
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is used in a notebook - will eventually be pushed into git as a notebook.\r\n",
        "\r\n",
        "import datetime\r\n",
        "from os.path import join, dirname\r\n",
        "from os import makedirs\r\n",
        "import tempfile\r\n",
        "from pyarrow import csv, parquet, string\r\n",
        "import ntpath\r\n",
        "\r\n",
        "\r\n",
        "output_delimiter = '\\t'\r\n",
        "\r\n",
        "\r\n",
        "class SchemaHandler(object):\r\n",
        "\r\n",
        "    def __init__(self, definitions):\r\n",
        "        self.feclookup = definitions\r\n",
        "        self.schema_cache : dict[str, dict[str, list[str]]]= {}  # file_version => (line type => schema)\r\n",
        "\r\n",
        "    def get_schema(self, version, linetype):\r\n",
        "        linetype = linetype.upper()\r\n",
        "        if linetype in ['H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7']:\r\n",
        "            # handle weird data bug\r\n",
        "            linetype = 'S' + linetype\r\n",
        "        \r\n",
        "        versioned_formdata = self.feclookup['v' + version]\r\n",
        "        i = len(linetype)\r\n",
        "\r\n",
        "        while i >= 0 and linetype[:i] not in versioned_formdata:\r\n",
        "            i -= 1\r\n",
        "\r\n",
        "        if not linetype:\r\n",
        "            raise Exception(\"Could not match linetype {0} on version {1}\".format(linetype, version))\r\n",
        "\r\n",
        "        final_key = linetype[:i]\r\n",
        "        return final_key, versioned_formdata.get(final_key, dict())\r\n",
        "\r\n",
        "    def get_schema_string(self, fileversion, clean_linetype):\r\n",
        "        if fileversion not in self.schema_cache:\r\n",
        "            self.schema_cache[fileversion] = {}\r\n",
        "\r\n",
        "        if clean_linetype not in self.schema_cache[fileversion]:\r\n",
        "            if clean_linetype == 'error':\r\n",
        "                self.schema_cache[fileversion][clean_linetype] = output_delimiter.join(['clean_linetype', 'upload_date', 'linetype', 'error', 'filename']).encode()\r\n",
        "            else:\r\n",
        "                _, schema = self.get_schema(fileversion, clean_linetype)\r\n",
        "                schema = list(schema)\r\n",
        "                schema.insert(0, 'upload_date')\r\n",
        "                schema.insert(0, 'clean_linetype')\r\n",
        "                schema.append('filename')\r\n",
        "                self.schema_cache[fileversion][clean_linetype] = output_delimiter.join(schema).encode()\r\n",
        "\r\n",
        "        final_value = self.schema_cache[fileversion][clean_linetype]\r\n",
        "        return final_value\r\n",
        "\r\n",
        "\r\n",
        "class LowMemoryFecFileParser(object):\r\n",
        "    \"\"\"\r\n",
        "    Given a file from the FEC, apply correct definitions.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, schema_handler, upload_date, line_aggregator):\r\n",
        "        self.schema_handler = schema_handler\r\n",
        "        self.upload_date = upload_date\r\n",
        "        self.line_aggregator = line_aggregator\r\n",
        "        self.schema_cache : dict[str, dict[str, list[str]]]= {}  # file_version => (line type => schema)\r\n",
        "\r\n",
        "    def processfile(self, filehandle, filename):\r\n",
        "        \"\"\"\r\n",
        "        Process all lines of a file and list of dictionaries, one per line.\r\n",
        "        \"\"\"\r\n",
        "        first_line = filehandle.readline()\r\n",
        "        first_line = first_line.replace('\"', '').strip().split(chr(28))\r\n",
        "        if first_line[0] != \"HDR\":\r\n",
        "            raise Exception(\"Failed to parse: HDR expected on first line\")\r\n",
        "\r\n",
        "        fileversion = first_line[2].strip()\r\n",
        "\r\n",
        "        in_comment = False\r\n",
        "\r\n",
        "        for line in filehandle:\r\n",
        "            line = line.strip()\r\n",
        "            line = line.replace('\"', '')\r\n",
        "\r\n",
        "            if not line:\r\n",
        "                continue\r\n",
        "            \r\n",
        "            if line == '[BEGINTEXT]':\r\n",
        "                in_comment = True\r\n",
        "                continue\r\n",
        "            elif in_comment:\r\n",
        "                if line == '[ENDTEXT]':\r\n",
        "                    in_comment = False\r\n",
        "                continue\r\n",
        "\r\n",
        "            line : list[str] = line.split(chr(28))\r\n",
        "            line = [l.replace(output_delimiter, ' ') for l in line]\r\n",
        "            linetype = line[0]\r\n",
        "            clean_linetype, schema = self.schema_handler.get_schema(fileversion, linetype)\r\n",
        "            \r\n",
        "            # Send the line to right place.\r\n",
        "            if schema:\r\n",
        "                if len(schema) < len(line):\r\n",
        "                    line = line[:len(schema)]\r\n",
        "                while len(line) < len(schema):\r\n",
        "                    line.append('')\r\n",
        "\r\n",
        "                line.insert(0, self.upload_date)\r\n",
        "                line.insert(0, clean_linetype)\r\n",
        "                line.append(ntpath.basename(filename))\r\n",
        "            else:\r\n",
        "                clean_linetype = 'error'\r\n",
        "                print(f\"Error row: {line}\")\r\n",
        "                line = [clean_linetype, self.upload_date, linetype, \"NoSchema\", filename]\r\n",
        "\r\n",
        "            line_out = output_delimiter.join(line)\r\n",
        "            self.write_line(clean_linetype, line_out, fileversion)\r\n",
        "\r\n",
        "    def summarize_schema_cache(self):\r\n",
        "        for file_version, cache in self.schema_cache.items():\r\n",
        "            for line_type, final_cache in cache.items():\r\n",
        "                cache_s = final_cache.decode().split(output_delimiter)\r\n",
        "                cache_size = len(cache_s)\r\n",
        "                print(f\"{file_version}, {line_type}, {cache_size}\")\r\n",
        "\r\n",
        "    def write_line(self, clean_linetype : str, line : str, fileversion : str):\r\n",
        "        self.line_aggregator.write(clean_linetype, line, fileversion)\r\n",
        "\r\n",
        "    def finalize(self):\r\n",
        "        self.line_aggregator.finalize()\r\n",
        "\r\n",
        "\r\n",
        "class LineAggregator(object):\r\n",
        "\r\n",
        "    def __init__(self, schema_handler, dateprefix, converter_handler):\r\n",
        "        self.schema_handler = schema_handler\r\n",
        "        self.dateprefix = dateprefix\r\n",
        "        self.file_pointers : dict[str, tempfile.TemporaryFile] = {}  # lineType -> fp\r\n",
        "        self.file_sizes : dict[str, int] = {}  # lineType -> current file size \r\n",
        "        self.converter_handler = converter_handler\r\n",
        "\r\n",
        "        self.file_size_limit = 1024 * 1024 * 200\r\n",
        "\r\n",
        "    def write(self, clean_linetype : str, line : str, file_version: str):\r\n",
        "        # Note that original_schema does not contain our added columns.\r\n",
        "        # It's used in converter to force null types to behave. Our added columns already do.\r\n",
        "        schema_str = self.schema_handler.get_schema_string(file_version, clean_linetype)\r\n",
        "        if not self._get_file(file_version, clean_linetype):\r\n",
        "            self._set_file(file_version, clean_linetype, schema_str)\r\n",
        "\r\n",
        "        line = line + '\\n'\r\n",
        "        line = line.encode()\r\n",
        "        self.file_sizes[clean_linetype] += self._get_file(file_version, clean_linetype).write(line)\r\n",
        "        \r\n",
        "        if self.file_sizes[clean_linetype] > self.file_size_limit:\r\n",
        "            _, original_schema = self.schema_handler.get_schema(file_version, clean_linetype)\r\n",
        "            self.converter_handler.convert(clean_linetype, self._get_file(file_version, clean_linetype), original_schema)\r\n",
        "            self._set_file(file_version, clean_linetype, schema_str)\r\n",
        "\r\n",
        "    def _get_file(self, file_version : str, clean_linetype : str):\r\n",
        "        version_pointers = self.file_pointers.get(file_version)\r\n",
        "        if not version_pointers:\r\n",
        "            return None\r\n",
        "        return version_pointers.get(clean_linetype)\r\n",
        "\r\n",
        "    def _set_file(self, fileversion : str, clean_linetype : str, schema_str : str):\r\n",
        "        if fileversion not in self.file_pointers:\r\n",
        "            self.file_pointers[fileversion] = {}\r\n",
        "\r\n",
        "        if clean_linetype in self.file_pointers[fileversion]:\r\n",
        "            del self.file_pointers[fileversion][clean_linetype]\r\n",
        "\r\n",
        "        local_file_handle = tempfile.TemporaryFile()\r\n",
        "\r\n",
        "        self.file_sizes[clean_linetype] = local_file_handle.write(schema_str)\r\n",
        "        local_file_handle.write('\\n'.encode())\r\n",
        "        self.file_pointers[fileversion][clean_linetype] = local_file_handle\r\n",
        "        \r\n",
        "        return local_file_handle\r\n",
        "\r\n",
        "    def finalize(self):\r\n",
        "        for fileversion, pointers in self.file_pointers.items():\r\n",
        "            for clean_linetype, file_pointer in pointers.items():\r\n",
        "                _, original_schema = self.schema_handler.get_schema(fileversion, clean_linetype)\r\n",
        "                self.converter_handler.convert(clean_linetype, file_pointer, original_schema)\r\n",
        "        self.file_pointers = {}\r\n",
        "        self.file_sizes = {}\r\n",
        "\r\n",
        "\r\n",
        "class ParquetConverter(object):\r\n",
        "\r\n",
        "    def __init__(self, root_folder, date_pattern) -> None:\r\n",
        "        self.root_folder = root_folder\r\n",
        "        self.date_pattern = date_pattern\r\n",
        "        self.counter = 0\r\n",
        "        self.files = {}  # line type => filenames\r\n",
        "\r\n",
        "    def convert(self, line_type: str, file_pointer, original_schema):\r\n",
        "        \"\"\"Convert a delimited file to parquet\"\"\"\r\n",
        "        file_pointer.flush()\r\n",
        "        file_pointer.seek(0)\r\n",
        "\r\n",
        "        column_opts_dict = {}\r\n",
        "        for col in original_schema:\r\n",
        "            column_opts_dict[col] = string()\r\n",
        "\r\n",
        "        try:\r\n",
        "            df = csv.read_csv(\r\n",
        "                file_pointer, \r\n",
        "                parse_options=csv.ParseOptions(delimiter=output_delimiter), \r\n",
        "                convert_options=csv.ConvertOptions(column_types=column_opts_dict)\r\n",
        "            )\r\n",
        "        except Exception:\r\n",
        "            print(f\"Failed on {line_type}. Dumping\")\r\n",
        "\r\n",
        "            emh = open('/tmp/emergency_dump.csv', 'wb')\r\n",
        "            file_pointer.seek(0)\r\n",
        "            emh.write(file_pointer.read())\r\n",
        "            emh.close()\r\n",
        "\r\n",
        "            raise\r\n",
        "\r\n",
        "        local_filename = join(self.root_folder, line_type, f'{self.date_pattern}_{self.counter}.snappy.parquet')\r\n",
        "        self.counter += 1\r\n",
        "        makedirs(dirname(local_filename), exist_ok=True)\r\n",
        "        new_fp = open(local_filename, 'wb')\r\n",
        "\r\n",
        "        parquet.write_table(df, new_fp, flavor='spark')\r\n",
        "        new_fp.close()\r\n",
        "        return local_filename   \r\n",
        "\r\n",
        "\r\n",
        "    def consolidate(self):\r\n",
        "        \"\"\"For each line type, consolidate the folder to a single parquet file and remove nulls\"\"\"\r\n",
        "        pass\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def build_parser(fec_definitions, parquet_root, date_pattern):\r\n",
        "    utc_timestamp = str(datetime.datetime.utcnow())\r\n",
        "    schema_handler = SchemaHandler(fec_definitions)\r\n",
        "    parquet_convert = ParquetConverter(parquet_root, date_pattern)\r\n",
        "    line_aggregator = LineAggregator(schema_handler, date_pattern, parquet_convert)\r\n",
        "    return LowMemoryFecFileParser(schema_handler, utc_timestamp, line_aggregator)\r\n",
        "\r\n",
        "\r\n",
        "def upload(local_path):\r\n",
        "    remote_path = f'amluploads/electronic/'\r\n",
        "    print(f'Uploading {local_path} to {remote_path}')\r\n",
        "    output_datastore.upload(local_path, remote_path, overwrite=True, show_progress=False)\r\n",
        "\r\n",
        "\r\n",
        "def get_tempdir(reason):\r\n",
        "    td = tempfile.TemporaryDirectory()\r\n",
        "    print(f\"Got {td.name} for {reason}\")\r\n",
        "    return td\r\n",
        "\r\n",
        "def process_file(files, datepattern, unzip_tempdir):\r\n",
        "    parquet_folder = get_tempdir(\"parquet_folder\")\r\n",
        "    parser = build_parser(fec_definitions, parquet_folder.name, datepattern)\r\n",
        "\r\n",
        "    print(f\"Starting to operate on {len(files)} files\")\r\n",
        "    try:\r\n",
        "        for rawfilename in files:\r\n",
        "            # run in blocks of 100 files, or otherwise watch for full disk. On full/100 then run convert/upload then cleanup/recreate\r\n",
        "            # your temp space.\r\n",
        "            filename = join(unzip_tempdir.name, rawfilename)\r\n",
        "            fh = open(filename, 'r', errors='ignore')\r\n",
        "            parser.processfile(fh, filename)\r\n",
        "        parser.finalize()\r\n",
        "        upload(parquet_folder.name)\r\n",
        "    finally:\r\n",
        "        print(\"Cleaning up\")\r\n",
        "        parquet_folder.cleanup()\r\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1662929163671
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\r\n",
        "import json\r\n",
        "\r\n",
        "import zipfile\r\n",
        "import tempfile\r\n",
        "import datetime\r\n",
        "\r\n",
        "dt = datetime.datetime(2021, 4, 20)\r\n",
        "while dt < datetime.datetime(2021, 5, 1):\r\n",
        "    datepattern = datetime.datetime.strftime(dt, '%Y%m%d')\r\n",
        "    print(f\"Working on {datepattern}\")\r\n",
        "\r\n",
        "    ds = Dataset.File.from_files((datastore, f'electronic/{datepattern}.zip'))\r\n",
        "    downloaded_files = ds.download()\r\n",
        "\r\n",
        "    unzip_tempdir = get_tempdir(\"rawdownload\")\r\n",
        "    with zipfile.ZipFile(open(downloaded_files[0], 'rb')) as zipObj:\r\n",
        "        zipObj.extractall(unzip_tempdir.name)\r\n",
        "\r\n",
        "    fec_definitions = json.loads(open(local_metadata_dataset, 'r').read())\r\n",
        "    files = listdir(unzip_tempdir.name)\r\n",
        "    process_file(files, datepattern, unzip_tempdir)\r\n",
        "    unzip_tempdir.cleanup()\r\n",
        "\r\n",
        "    dt += datetime.timedelta(days=1)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Working on 20210420\nGot /tmp/tmpxwhg2kln for rawdownload\nGot /tmp/tmpau8y4345 for parquet_folder\nStarting to operate on 614 files\nUploading /tmp/tmpau8y4345 to amluploads/electronic/\nCleaning up\nWorking on 20210421\nGot /tmp/tmpqmqke7d_ for rawdownload\nGot /tmp/tmp82m3vlh9 for parquet_folder\nStarting to operate on 170 files\nUploading /tmp/tmp82m3vlh9 to amluploads/electronic/\nCleaning up\nWorking on 20210422\nGot /tmp/tmpx4ukmhlh for rawdownload\nGot /tmp/tmptnz4y8nz for parquet_folder\nStarting to operate on 109 files\nUploading /tmp/tmptnz4y8nz to amluploads/electronic/\nCleaning up\nWorking on 20210423\nGot /tmp/tmp0ehlslkb for rawdownload\nGot /tmp/tmpt2rv4l7g for parquet_folder\nStarting to operate on 95 files\nUploading /tmp/tmpt2rv4l7g to amluploads/electronic/\nCleaning up\nWorking on 20210424\nGot /tmp/tmpjjgpuody for rawdownload\nGot /tmp/tmptdlkb8ms for parquet_folder\nStarting to operate on 33 files\nUploading /tmp/tmptdlkb8ms to amluploads/electronic/\nCleaning up\nWorking on 20210425\nGot /tmp/tmph86zk6sq for rawdownload\nGot /tmp/tmpag9mf_vz for parquet_folder\nStarting to operate on 6 files\nUploading /tmp/tmpag9mf_vz to amluploads/electronic/\nCleaning up\nWorking on 20210426\nGot /tmp/tmporyvdgvs for rawdownload\nGot /tmp/tmparm29j2h for parquet_folder\nStarting to operate on 88 files\nUploading /tmp/tmparm29j2h to amluploads/electronic/\nCleaning up\nWorking on 20210427\nGot /tmp/tmp6qvblqu6 for rawdownload\nGot /tmp/tmph8jldl3c for parquet_folder\nStarting to operate on 97 files\nUploading /tmp/tmph8jldl3c to amluploads/electronic/\nCleaning up\nWorking on 20210428\nGot /tmp/tmpzdujzian for rawdownload\nGot /tmp/tmpwu0fueyf for parquet_folder\nStarting to operate on 117 files\nUploading /tmp/tmpwu0fueyf to amluploads/electronic/\nCleaning up\nWorking on 20210429\nGot /tmp/tmp4v3m_v9t for rawdownload\nGot /tmp/tmpd2udzmhz for parquet_folder\nStarting to operate on 115 files\nUploading /tmp/tmpd2udzmhz to amluploads/electronic/\nCleaning up\nWorking on 20210430\nGot /tmp/tmp5f5ewe3j for rawdownload\nGot /tmp/tmpxdamkkbq for parquet_folder\nStarting to operate on 78 files\nUploading /tmp/tmpxdamkkbq to amluploads/electronic/\nCleaning up\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1662929362989
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}